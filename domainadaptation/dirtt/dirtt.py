import tensorflow as tf
from ._vada_dirtt import _VADA_DIRTT

class DIRTT(_VADA_DIRTT):
    def __init__(self, image_input, feature_output, domain_output, classification_output, name='dirt-t', **kwargs):
        super(DIRTT, self).__init__(image_input, feature_output, domain_output, classification_output, 
                                   name=name, **kwargs)

    def _update_teacher(self, ref_model):
        self._ema.apply(ref_model.trainable_variables)
        for w_t,w_r in zip(self._teacher.trainable_variables, ref_model.trainable_variables):
            w_t.assign(self._ema.average(w_r))
        
    def _loss(self, x_target, pseudo_y_target, f_t, d_t, y_t):
           
        # ensure parameterization-invariant, in which KL(h(x; theta)||h(x; theta+d theta)) < epsilon
        L_y = self.compiled_loss(y_t, pseudo_y_target, regularization_losses=None)
        
        L_c = tf.reduce_mean(self.loss(y_t, y_t)) # conditional entropy 
        L_v_t = self._vat_loss(x_target, y_t) # locally-Lipschitz constraint
        
        # losses are multiplied by coefficients
        L_y = self.beta*L_y
        L_c, L_v_t = self.lambda_t*L_c, self.lambda_t*L_v_t
            
        return (L_y, L_c, L_v_t)
            
    def call(self, x_target, training=False):
        if training:
            # pseudo labels generated by teacher that weights are updated by exponential moving average
            # exponential moving average ensure parameterization-invariant
            pseudo_y_target = tf.stop_gradient(self._teacher(x_target, training=False))
            
            # compute feature map, domain prediction and label prediction
            f_t, d_t, y_t = self.model(x_target, training=True)
        
            # compute losses
            losses = self._loss(x_target, pseudo_y_target, f_t, d_t, y_t)
            
            L_y, L_c, L_v_t = losses
            
            self.add_loss(L_y)
            self.add_loss(L_c)
            self.add_loss(L_v_t)
            
            L_all = sum(losses)
            self.add_metric(L_all, name='L_all')
            self.add_metric(L_y, name='L_y')
            self.add_metric(L_c, name='L_c')
            self.add_metric(L_v_t, name='L_v_t')
            
            return pseudo_y_target, y_t
            
        else:
            return self.model(x_source)
    
    def train_step(self, data):
        if len(data)==3:
            _, _, x_target = data
        else:
            x_target = data
            
        self._update_teacher(self.classifier) # updated by exponential moving average
        
        with tf.GradientTape(persistent=True) as tape:
            pseudo_y, pred_y = self(x_target, training=True)
            L = sum(self.losses)
        
        self.optimizers['t'].minimize(L, self.classifier.trainable_variables, tape=tape)
        self.compiled_metrics.update_state(pseudo_y, pred_y)
        
        return_metrics = {}
        for metric in self.metrics:
            result = metric.result()
            if isinstance(result, dict):
                return_metrics.update(result)
            else:
                return_metrics[metric.name] = result
            
        return return_metrics  
    
